{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all models within a loop\n",
    "The loop below iterates through directories to extract models, find the number of layers within a model, and store that information in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'aubrey.ipynb',\n",
       " 'Cricket Image Classification Presentation.pptx',\n",
       " 'dendrogram.png',\n",
       " 'logistic_regression.Rmd',\n",
       " 'models.csv',\n",
       " 'model_clustering.ipynb',\n",
       " 'model_data.csv',\n",
       " 'model_download.py',\n",
       " 'model_extraction.ipynb']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('pretrained_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all model names\n",
    "model_list = [\n",
    "'rohitsroch-hybrid_utt-clusterrank_bart-base_samsum_sum',\n",
    "'bookbot-distil-wav2vec2-xls-r-adult-child-cls-89m',\n",
    "'imadd-segformer-b0-finetuned-segments-water-2',\n",
    "'arahmi6-distilbert-base-uncased-finetuned-cola',\n",
    "'lgrobol-m2m100_418M_br_fr',\n",
    "'npleshkanov-ru-labse-toxic',\n",
    "'anas-awadalla-albert-xl-v2-finetuned-squad',\n",
    "'philschmid-MiniLMv2-L12-H384-sst2',\n",
    "'deepset-deberta-v3-base-squad2',\n",
    "'enoriega-kw_pubmed_1000_0.0003',\n",
    "'krinal214-zero_shot',\n",
    "'GItaf-roberta-base-roberta-base-TF-weight1-epoch5',\n",
    "'cj-mills-distilbert-base-uncased-finetuned-emotion',\n",
    "'allenai-System2_FigLang2022',\n",
    "'monakth-distilbert-base-cased-finetuned-squad',\n",
    "'everyl12-crisis_sentiment_roberta',\n",
    "'daveccampbell-xlm-roberta-base-finetuned-marc-en',\n",
    "'iis2009002-xlm-roberta-base-finetuned-panx-all',\n",
    "'HrayrMSint-distilbert-base-uncased-finetuned-clinc',\n",
    "'louisdeco-camembert-base-finetuned-RankLineCause',\n",
    "'cammy-led-large-16384-arxiv-100-MDS',\n",
    "'Graphcore-bert-base-uncased-squad',\n",
    "'tbosse-distilbert-base-uncased-finetuned-pos',\n",
    "'theojolliffe-bart-cnn-pubmed-arxiv-pubmed-arxiv',\n",
    "'KarelDO-bert-base-uncased.CEBaB_confounding.price_food_ambiance_negative.sa.5-class.seed_43',\n",
    "'anas-awadalla-spanbert-base-cased-few-shot-k-256-finetuned-squad-seed-8',\n",
    "'furyhawk-t5-small-finetuned-bbc',\n",
    "'yinde-fatimah_fake_news_bert',\n",
    "'transformersbook-xlm-roberta-base-finetuned-panx-it',\n",
    "'dpasch01-finetune-data-skills',\n",
    "'mrm8488-bert-tiny-finetuned-enron-spam-detection',\n",
    "'Bistolero-nl3',\n",
    "'autoevaluate-glue-mnli',\n",
    "'speech31-wav2vec2-large-english-phoneme-v2',\n",
    "'theojolliffe-pegasus-cnn_dailymail-finetuned-roundup',\n",
    "'anas-awadalla-spanbert-base-cased-few-shot-k-512-finetuned-squad-seed-4',\n",
    "'gchhablani-fnet-large-finetuned-cola-copy4',\n",
    "'Seongmi-kobigbird-bert-base-finetuned-klue-v2',\n",
    "'subhasisj-Zh-Mulitlingual-MiniLM',\n",
    "'charlieoneill-distilbert-base-uncased-gradient-clinic',\n",
    "'HPL-distilbert-base-uncased-finetuned-emotion',\n",
    "'anuragshas-wav2vec2-xls-r-300m-mr-cv8-with-lm',\n",
    "'VanessaSchenkel-unicamp-finetuned-en-to-pt-dataset-ted',\n",
    "'Aleksandar-electra-srb-oscar',\n",
    "'aditeyabaral-sentencetransformer-indic-bert',\n",
    "'nntadotzip-xlnet-base-cased-IUChatbot-ontologyDts-BertPretrainedTokenizerFast',\n",
    "'gabrielsgaspar-test-trainer',\n",
    "'Luttufuttu-finetuning-sentiment-model-3000-samples',\n",
    "'anas-awadalla-t5-small-few-shot-k-256-finetuned-squad-seed-0',\n",
    "'flboehm-reddit-bert-text3',\n",
    "'Lindeberg-distilbert-base-uncased-finetuned-cola',\n",
    "'tbosse-bert-base-german-cased-finetuned-subj_v3',\n",
    "'DataIntelligenceTeam-Rhenus-Annotations-vgg-json',\n",
    "'GItaf-roberta-base-roberta-base-TF-weight1-epoch15',\n",
    "'NFflow-healthcare_27.03.2021-27.03.2022_redditflow',\n",
    "'m3hrdadfi-xlmr-large-qa-fa',\n",
    "'zhiguoxu-chinese-macbert-base-finetuned-ner',\n",
    "'zluvolyote-CUBERT',\n",
    "'scasutt-wav2vec2-large-xlsr-53_toy_train_data',\n",
    "'sibyl-BART-large-commongen',\n",
    "'markt23917-finetuning-sentiment-model-3000-samples',\n",
    "'Khalsuu-english-filipino-wav2vec2-l-xls-r-test-09',\n",
    "'mrm8488-convnext-tiny-finetuned-eurosat',\n",
    "'philschmid-distilbert-base-multilingual-cased-sentiment-2',\n",
    "'Abonia-finetuning-sentiment-model-3000-samples',\n",
    "'husnu-xtremedistil-l6-h256-uncased-TQUAD-finetuned_lr-2e-05_epochs-6',\n",
    "'milyiyo-electra-base-gen-finetuned-amazon-review',\n",
    "'saghar-TinyBERT_L-4_H-312_v2-finetuned-wikitext103',\n",
    "'ceggian-sbert_pt_reddit_softmax_32',\n",
    "'Jeska-BertjeWDialDataALLQonly07',\n",
    "'horsbug98-Part_2_mBERT_Model_E1',\n",
    "'yazdipour-sparql-qald9-t5-base-2021-10-19_00-15',\n",
    "'YeRyeongLee-bert-base-uncased-finetuned-small-0505',\n",
    "'platzi-platzi-distilroberta-base-mrpc-glue-yeder-lvicente',\n",
    "'wesleyaag-data2vec-squad-test',\n",
    "'akadriu-wav2vec2-large-xlsr-53-Total2e-4_4',\n",
    "'pitspits-xlm-roberta-base-finetuned-panx-de',\n",
    "'yazdipour-text-to-sparql-t5-small',\n",
    "'beautifulpichai-all-MiniLM-L12-v2-ledgar-contrastive',\n",
    "'DrishtiSharma-xls-r-es-test-lm-finetuned-sentiment-mesd','anas-awadalla-bert-base-uncased-few-shot-k-1024-finetuned-squad-seed-6']\n",
    "#model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohitsroch-hybrid_utt-clusterrank_bart-base_samsum_sum\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'pretrained_models/rohitsroch-hybrid_utt-clusterrank_bart-base_samsum_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_models/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m item\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(item)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'pretrained_models/rohitsroch-hybrid_utt-clusterrank_bart-base_samsum_sum'"
     ]
    }
   ],
   "source": [
    "# print file name within each individual directory\n",
    "for item in model_list:\n",
    "    path = 'pretrained_models/' + item\n",
    "    print(item)\n",
    "    print(os.listdir(path))\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rohitsroch-hybrid_utt-clusterrank_bart-base_samsum_sum\n",
      "bookbot-distil-wav2vec2-xls-r-adult-child-cls-89m\n",
      "imadd-segformer-b0-finetuned-segments-water-2\n",
      "arahmi6-distilbert-base-uncased-finetuned-cola\n",
      "lgrobol-m2m100_418M_br_fr\n",
      "npleshkanov-ru-labse-toxic\n",
      "anas-awadalla-albert-xl-v2-finetuned-squad\n",
      "philschmid-MiniLMv2-L12-H384-sst2\n",
      "deepset-deberta-v3-base-squad2\n",
      "enoriega-kw_pubmed_1000_0.0003\n",
      "krinal214-zero_shot\n",
      "GItaf-roberta-base-roberta-base-TF-weight1-epoch5\n",
      "cj-mills-distilbert-base-uncased-finetuned-emotion\n",
      "allenai-System2_FigLang2022\n",
      "monakth-distilbert-base-cased-finetuned-squad\n",
      "everyl12-crisis_sentiment_roberta\n",
      "daveccampbell-xlm-roberta-base-finetuned-marc-en\n",
      "iis2009002-xlm-roberta-base-finetuned-panx-all\n",
      "HrayrMSint-distilbert-base-uncased-finetuned-clinc\n",
      "louisdeco-camembert-base-finetuned-RankLineCause\n",
      "cammy-led-large-16384-arxiv-100-MDS\n",
      "Graphcore-bert-base-uncased-squad\n",
      "tbosse-distilbert-base-uncased-finetuned-pos\n",
      "theojolliffe-bart-cnn-pubmed-arxiv-pubmed-arxiv\n",
      "KarelDO-bert-base-uncased.CEBaB_confounding.price_food_ambiance_negative.sa.5-class.seed_43\n",
      "anas-awadalla-spanbert-base-cased-few-shot-k-256-finetuned-squad-seed-8\n",
      "furyhawk-t5-small-finetuned-bbc\n",
      "yinde-fatimah_fake_news_bert\n",
      "transformersbook-xlm-roberta-base-finetuned-panx-it\n",
      "dpasch01-finetune-data-skills\n",
      "mrm8488-bert-tiny-finetuned-enron-spam-detection\n",
      "Bistolero-nl3\n",
      "autoevaluate-glue-mnli\n",
      "speech31-wav2vec2-large-english-phoneme-v2\n",
      "theojolliffe-pegasus-cnn_dailymail-finetuned-roundup\n",
      "anas-awadalla-spanbert-base-cased-few-shot-k-512-finetuned-squad-seed-4\n",
      "gchhablani-fnet-large-finetuned-cola-copy4\n",
      "Seongmi-kobigbird-bert-base-finetuned-klue-v2\n",
      "subhasisj-Zh-Mulitlingual-MiniLM\n",
      "charlieoneill-distilbert-base-uncased-gradient-clinic\n",
      "HPL-distilbert-base-uncased-finetuned-emotion\n",
      "anuragshas-wav2vec2-xls-r-300m-mr-cv8-with-lm\n",
      "VanessaSchenkel-unicamp-finetuned-en-to-pt-dataset-ted\n",
      "Aleksandar-electra-srb-oscar\n",
      "aditeyabaral-sentencetransformer-indic-bert\n",
      "nntadotzip-xlnet-base-cased-IUChatbot-ontologyDts-BertPretrainedTokenizerFast\n",
      "gabrielsgaspar-test-trainer\n",
      "Luttufuttu-finetuning-sentiment-model-3000-samples\n",
      "anas-awadalla-t5-small-few-shot-k-256-finetuned-squad-seed-0\n",
      "flboehm-reddit-bert-text3\n",
      "Lindeberg-distilbert-base-uncased-finetuned-cola\n",
      "tbosse-bert-base-german-cased-finetuned-subj_v3\n",
      "DataIntelligenceTeam-Rhenus-Annotations-vgg-json\n",
      "GItaf-roberta-base-roberta-base-TF-weight1-epoch15\n",
      "NFflow-healthcare_27.03.2021-27.03.2022_redditflow\n",
      "m3hrdadfi-xlmr-large-qa-fa\n",
      "zhiguoxu-chinese-macbert-base-finetuned-ner\n",
      "zluvolyote-CUBERT\n",
      "scasutt-wav2vec2-large-xlsr-53_toy_train_data\n",
      "sibyl-BART-large-commongen\n",
      "markt23917-finetuning-sentiment-model-3000-samples\n",
      "Khalsuu-english-filipino-wav2vec2-l-xls-r-test-09\n",
      "mrm8488-convnext-tiny-finetuned-eurosat\n",
      "philschmid-distilbert-base-multilingual-cased-sentiment-2\n",
      "Abonia-finetuning-sentiment-model-3000-samples\n",
      "husnu-xtremedistil-l6-h256-uncased-TQUAD-finetuned_lr-2e-05_epochs-6\n",
      "milyiyo-electra-base-gen-finetuned-amazon-review\n",
      "saghar-TinyBERT_L-4_H-312_v2-finetuned-wikitext103\n",
      "ceggian-sbert_pt_reddit_softmax_32\n",
      "Jeska-BertjeWDialDataALLQonly07\n",
      "horsbug98-Part_2_mBERT_Model_E1\n",
      "yazdipour-sparql-qald9-t5-base-2021-10-19_00-15\n",
      "YeRyeongLee-bert-base-uncased-finetuned-small-0505\n",
      "platzi-platzi-distilroberta-base-mrpc-glue-yeder-lvicente\n",
      "wesleyaag-data2vec-squad-test\n",
      "akadriu-wav2vec2-large-xlsr-53-Total2e-4_4\n",
      "pitspits-xlm-roberta-base-finetuned-panx-de\n",
      "yazdipour-text-to-sparql-t5-small\n",
      "beautifulpichai-all-MiniLM-L12-v2-ledgar-contrastive\n",
      "DrishtiSharma-xls-r-es-test-lm-finetuned-sentiment-mesd\n",
      "anas-awadalla-bert-base-uncased-few-shot-k-1024-finetuned-squad-seed-6\n"
     ]
    }
   ],
   "source": [
    "# create empty df\n",
    "model_df = pd.DataFrame(columns=['model_name','num_layers'])\n",
    "total_weights = pd.DataFrame(columns=model_list)\n",
    "for item in model_list:\n",
    "    #print(item)\n",
    "    dirpath = 'pretrained_models/' + item\n",
    "    \n",
    "    # extract number of files within directory\n",
    "    dir_contents = os.listdir(dirpath)\n",
    "    num_files = len(dir_contents)\n",
    "    \n",
    "    # check if more than one file in directory\n",
    "    if num_files == 1:\n",
    "        # expand path to include file\n",
    "        filepath = dirpath + '/' + dir_contents[0]\n",
    "        model = torch.load(filepath, map_location = torch.device('cpu'))  # load model\n",
    "        # extract information from each model\n",
    "        num_layers = len(model)\n",
    "        test = [0] * 26\n",
    "        j=0\n",
    "        print(item)\n",
    "        for i in list(model.values())[0:26]:\n",
    "            test[j] = len(i.numpy())\n",
    "            j +=1\n",
    "        total_weights[item] = test\n",
    "        # add model name and num layers to df\n",
    "        new_item = pd.DataFrame([[item, num_layers]], columns=['model_name','num_layers'])\n",
    "        model_df = pd.concat([model_df, new_item], ignore_index=True)\n",
    "    else:\n",
    "        # iterate through all files in directory\n",
    "        for i in range(num_files):\n",
    "            # extract specific file name\n",
    "            file = dir_contents[i]\n",
    "            \n",
    "            # expand path to include file\n",
    "            filepath = dirpath + '/' + file\n",
    "            model = torch.load(filepath, map_location = torch.device('cpu'))  # load model\n",
    "            \n",
    "            # extract information from each model\n",
    "            num_layers = len(model)\n",
    "            test = [0] * 26\n",
    "            j = 0\n",
    "            for i in list(model.values())[0:26]:\n",
    "                test[j] = len(i.cpu().numpy())\n",
    "                j +=1\n",
    "            total_weights[item] = test\n",
    "            # add model name and num layers to df\n",
    "            new_item = pd.DataFrame([[item, num_layers]], columns=['model_name','num_layers'])\n",
    "            model_df = pd.concat([model_df, new_item], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_layers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>anas-awadalla-albert-xl-v2-finetuned-squad</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>aditeyabaral-sentencetransformer-indic-bert</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     model_name num_layers\n",
       "8    anas-awadalla-albert-xl-v2-finetuned-squad         26\n",
       "49  aditeyabaral-sentencetransformer-indic-bert         26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df[model_df[\"num_layers\"]==min(model_df[\"num_layers\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_df.to_csv('layers.csv')\n",
    "transpose = total_weights.T\n",
    "transpose.to_csv('weights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all models within a loop -- unique model names\n",
    "The loop below iterates through directories to extract models, find the number of layers within a model, and store that information in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all model names\n",
    "model_list = os.listdir('pretrained_models')[60:65]\n",
    "#model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print file name within each individual directory\n",
    "for item in model_list:\n",
    "    path = 'pretrained_models/' + item\n",
    "    print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty df\n",
    "model_df = pd.DataFrame(columns=['model_name','num_layers'])\n",
    "\n",
    "for item in model_list:\n",
    "    #print(item)\n",
    "    dirpath = 'pretrained_models/' + item\n",
    "    \n",
    "    # extract number of files within directory\n",
    "    dir_contents = os.listdir(dirpath)\n",
    "    num_files = len(dir_contents)\n",
    "    \n",
    "    # check if more than one file in directory\n",
    "    if num_files == 1:\n",
    "        # expand path to include file\n",
    "        filepath = dirpath + '/' + dir_contents[0]\n",
    "        model = torch.load(filepath, map_location = torch.device('cpu'))  # load model\n",
    "\n",
    "        # extract information from each model\n",
    "        num_layers = len(model)\n",
    "\n",
    "        # add model name and num layers to df\n",
    "        new_item = pd.DataFrame([[item, num_layers]], columns=['model_name','num_layers'])\n",
    "        model_df = pd.concat([model_df, new_item], ignore_index=True)\n",
    "    else:\n",
    "        # iterate through all files in directory\n",
    "        for i in range(num_files):\n",
    "            # extract specific file name\n",
    "            file = dir_contents[i]\n",
    "            \n",
    "            # expand path to include file\n",
    "            filepath = dirpath + '/' + file\n",
    "            model = torch.load(filepath, map_location = torch.device('cpu'))  # load model\n",
    "\n",
    "            # extract information from each model\n",
    "            num_layers = len(model)\n",
    "            \n",
    "            # create a unique name for specific model found within directory\n",
    "            filename = item + str(i)\n",
    "            \n",
    "            # add model name and num layers to df\n",
    "            new_item = pd.DataFrame([[filename, num_layers]], columns=['model_name','num_layers'])\n",
    "            model_df = pd.concat([model_df, new_item], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at contents of ordered dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print contents of ordered dictionary\n",
    "for key, value in model.items():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
